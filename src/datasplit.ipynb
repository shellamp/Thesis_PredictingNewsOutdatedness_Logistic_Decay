{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b9d9ea",
   "metadata": {},
   "source": [
    "**1st Data Split (All to Labelling and Probability)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Original t_bin counts:\n",
      "t_bin\n",
      "very_old      7900\n",
      "old           1555\n",
      "middle_age    1501\n",
      "recent         941\n",
      "fresh          580\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üéØ Target sample size per bin: 580\n",
      "\n",
      "üì¶ Final stratified bin counts:\n",
      "t_bin\n",
      "fresh         580\n",
      "recent        580\n",
      "middle_age    580\n",
      "old           580\n",
      "very_old      580\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ Selected articles: 2900\n",
      "üóÇÔ∏è Remaining articles: 9577\n",
      "\n",
      "‚úÖ Saved selected to: /Users/sheillaschool/Documents/final/Thesis_PredictingNewsOutdatedness_LogisticDecay/data/main_data/finetuning/finetuning.json\n",
      "‚úÖ Saved remaining to: /Users/sheillaschool/Documents/final/Thesis_PredictingNewsOutdatedness_LogisticDecay/data/main_data/probability/probability.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_PATH = \"/Users/sheillaschool/Documents/final/Thesis_PredictingNewsOutdatedness_LogisticDecay/data/main_data/main_dataset.json\"\n",
    "OUTPUT_PATH = \"/Users/sheillaschool/Documents/final/Thesis_PredictingNewsOutdatedness_LogisticDecay/data/main_data/finetuning/finetuning.json\"\n",
    "REMAINING_OUTPUT = \"/Users/sheillaschool/Documents/final/Thesis_PredictingNewsOutdatedness_LogisticDecay/data/main_data/probability/probability.json\"\n",
    "\n",
    "with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient=\"index\").reset_index().rename(columns={\"index\": \"article_id\"})\n",
    "\n",
    "bin_counts = df[\"t_bin\"].value_counts()\n",
    "print(\"‚úÖ Original t_bin counts:\")\n",
    "print(bin_counts)\n",
    "\n",
    "target_count = bin_counts.min()\n",
    "print(f\"\\nüéØ Target sample size per bin: {target_count}\")\n",
    "\n",
    "selected_rows = []\n",
    "bin_order = [\"fresh\", \"recent\", \"middle_age\", \"old\", \"very_old\"]\n",
    "bins_to_use = [b for b in bin_order if b in df[\"t_bin\"].unique()]\n",
    "\n",
    "for bin_label in bins_to_use:\n",
    "    bin_df = df[df[\"t_bin\"] == bin_label].copy()\n",
    "    unique_t_df = bin_df.drop_duplicates(subset=\"t\")\n",
    "\n",
    "    if len(unique_t_df) >= target_count:\n",
    "        selected_bin = unique_t_df.sample(n=target_count, random_state=42)\n",
    "    else:\n",
    "        extra_needed = target_count - len(unique_t_df)\n",
    "        remaining = bin_df[~bin_df.index.isin(unique_t_df.index)]\n",
    "        fill = remaining.sample(n=min(extra_needed, len(remaining)), random_state=42)\n",
    "        selected_bin = pd.concat([unique_t_df, fill], ignore_index=True)\n",
    "\n",
    "    selected_rows.append(selected_bin)\n",
    "\n",
    "final_df = pd.concat(selected_rows).drop_duplicates(subset=\"article_id\").reset_index(drop=True)\n",
    "\n",
    "selected_ids = set(final_df[\"article_id\"])\n",
    "remaining_df = df[~df[\"article_id\"].isin(selected_ids)].copy().reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüì¶ Final stratified bin counts:\")\n",
    "print(final_df[\"t_bin\"].value_counts())\n",
    "print(f\"\\n‚úÖ Selected articles: {len(final_df)}\")\n",
    "print(f\"üóÇÔ∏è Remaining articles: {len(remaining_df)}\")\n",
    "\n",
    "final_dict = {\n",
    "    str(row[\"article_id\"]): row.drop(labels=[\"article_id\"]).to_dict()\n",
    "    for _, row in final_df.iterrows()\n",
    "}\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "remaining_dict = {\n",
    "    str(row[\"article_id\"]): row.drop(labels=[\"article_id\"]).to_dict()\n",
    "    for _, row in remaining_df.iterrows()\n",
    "}\n",
    "with open(REMAINING_OUTPUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(remaining_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved selected to: {OUTPUT_PATH}\")\n",
    "print(f\"‚úÖ Saved remaining to: {REMAINING_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b51f4",
   "metadata": {},
   "source": [
    "**2nd Data Split (Labelling to Train/Val/Test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b52d483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: [(1, 830), (0, 2070)]\n",
      "üîç Undersampling majority class '0' to match minority class '1' (830 samples)\n",
      "üìä Train label distribution: {1: 498, 0: 498}\n",
      "üìä Validation label distribution: {1: 166, 0: 166}\n",
      "üìä Test label distribution: {0: 166, 1: 166}\n",
      "üßæ Remaining majority samples (not in balanced seed): 1240\n",
      "‚úÖ Saved: train_balanced.json (996 samples)\n",
      "‚úÖ Saved: val_balanced.json (332 samples)\n",
      "‚úÖ Saved: test_balanced.json (332 samples)\n",
      "‚úÖ Saved: remaining_majority_not_in_seed.json (1240 samples)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# === CONFIG ===\n",
    "INPUT_PATH = Path(\"/Users/sheillaschool/Documents/final/Thesis_PredictingNewsOutdatedness_Logistic_Decay/model/labelling/data/manual/rulebased_manual.json\")\n",
    "OUTPUT_DIR = INPUT_PATH.parent / \"balanced_split\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TRAIN_OUTPUT = OUTPUT_DIR / \"train_balanced.json\"\n",
    "VAL_OUTPUT = OUTPUT_DIR / \"val_balanced.json\"\n",
    "TEST_OUTPUT = OUTPUT_DIR / \"test_balanced.json\"\n",
    "EXTRA_OUTPUT = OUTPUT_DIR / \"remaining_majority_not_in_seed.json\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# === HELPERS ===\n",
    "def load_json(path: Path) -> dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data: dict, path: Path) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved: {path.name} ({len(data)} samples)\")\n",
    "\n",
    "def print_label_distribution(name: str, items: list) -> None:\n",
    "    labels = [v[\"label\"] for _, v in items]\n",
    "    counter = Counter(labels)\n",
    "    print(f\"üìä {name} label distribution: {dict(counter)}\")\n",
    "\n",
    "# === MAIN ===\n",
    "def main():\n",
    "    data = load_json(INPUT_PATH)\n",
    "\n",
    "    # === Separate by class ===\n",
    "    label_groups = {}\n",
    "    for k, v in data.items():\n",
    "        label = v.get(\"label\")\n",
    "        if label not in label_groups:\n",
    "            label_groups[label] = []\n",
    "        label_groups[label].append((k, v))\n",
    "\n",
    "    # === Identify minority and majority classes ===\n",
    "    sorted_labels = sorted(label_groups.items(), key=lambda x: len(x[1]))\n",
    "    minority_label, minority_items = sorted_labels[0]\n",
    "    majority_label, majority_items = sorted_labels[1]\n",
    "    target_size = len(minority_items)\n",
    "\n",
    "    print(f\"Class distribution: {[(l, len(v)) for l, v in label_groups.items()]}\")\n",
    "    print(f\"üîç Undersampling majority class '{majority_label}' to match minority class '{minority_label}' ({target_size} samples)\")\n",
    "\n",
    "    # === Undersample majority ===\n",
    "    majority_sampled = random.sample(majority_items, target_size)\n",
    "    balanced_items = minority_items + majority_sampled\n",
    "    random.shuffle(balanced_items)\n",
    "\n",
    "    # === Train/Test Split ===\n",
    "    labels = [v[\"label\"] for _, v in balanced_items]\n",
    "    train_items, test_items, train_labels, _ = train_test_split(\n",
    "        balanced_items, labels, test_size=0.20, stratify=labels, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # === Train ‚Üí Train/Val ===\n",
    "    train_items_final, val_items, _, _ = train_test_split(\n",
    "        train_items, train_labels, test_size=0.25, stratify=train_labels, random_state=RANDOM_SEED\n",
    "    )\n",
    "    # Final: 60% train, 20% val, 20% test\n",
    "\n",
    "    # === Save leftover majority class samples not used ===\n",
    "    used_keys = set(k for k, _ in balanced_items)\n",
    "    remaining_majority = [(k, v) for k, v in majority_items if k not in used_keys]\n",
    "\n",
    "    # === Report ===\n",
    "    print_label_distribution(\"Train\", train_items_final)\n",
    "    print_label_distribution(\"Validation\", val_items)\n",
    "    print_label_distribution(\"Test\", test_items)\n",
    "    print(f\"üßæ Remaining majority samples (not in balanced seed): {len(remaining_majority)}\")\n",
    "\n",
    "    # === Save ===\n",
    "    save_json({k: v for k, v in train_items_final}, TRAIN_OUTPUT)\n",
    "    save_json({k: v for k, v in val_items}, VAL_OUTPUT)\n",
    "    save_json({k: v for k, v in test_items}, TEST_OUTPUT)\n",
    "    save_json({k: v for k, v in remaining_majority}, EXTRA_OUTPUT)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
